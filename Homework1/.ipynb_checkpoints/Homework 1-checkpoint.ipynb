{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Task-1.-Compiling-Ebola-Data\"><span class=\"toc-item-num\">Task 1.&nbsp;&nbsp;</span>Compiling Ebola Data</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-2.-RNA-Sequences\"><span class=\"toc-item-num\">Task 2.&nbsp;&nbsp;</span>RNA Sequences</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-3.-Class-War-in-Titanic\"><span class=\"toc-item-num\">Task 3.&nbsp;&nbsp;</span>Class War in Titanic</a></div></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "DATA_FOLDER = 'Data' # Use the data folder provided in Tutorial 02 - Intro to Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Compiling Ebola Data\n",
    "\n",
    "The `DATA_FOLDER/ebola` folder contains summarized reports of Ebola cases from three countries (Guinea, Liberia and Sierra Leone) during the recent outbreak of the disease in West Africa. For each country, there are daily reports that contain various information about the outbreak in several cities in each country.\n",
    "\n",
    "Use pandas to import these data files into a single `Dataframe`.\n",
    "Using this `DataFrame`, calculate for *each country*, the *daily average per month* of *new cases* and *deaths*.\n",
    "Make sure you handle all the different expressions for *new cases* and *deaths* that are used in the reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Report \n",
    "The first step was to import all the data from the ebola data floder. We created a different list of path for each country and then concatenated all the fill to create one dataframe per country.\n",
    "The second step was to look at the data to find relevant columns, value, etc... to create a Dataframe that will contains the more informative data.\n",
    "\n",
    "The first thing that we noticed is that real-life data is messy! Each country used different labels, format, value or even how they treat a missing value. Moreover it is easy to see that the data contains \"mistakes\" that can totally change our final summary data. One stiking example resides in the Liberia data, where the cumulative value of cases in december are written in the new cases rows. Missing that incoherence would lead to totally wrong statistics.\n",
    "\n",
    "Based on those observations we did choices on what value we were going to use.\n",
    "We decided to chose values that looked \"stables\". In the sens where we choosed values that we evaluate as realistic and that minimizes outling data.\n",
    "\n",
    "Our choices are the following:\n",
    "\n",
    "    -For the Guinea data: The data of guinea was quite clean. We chose the variable \"New deaths registered\" for the deaths and \"new cases of confirmed\" for the new cases.\n",
    "    This choice was based on the fact that for the deaths the data had no suspect value and that the field was present on all daily report. Concerning the new cases, we've made the choice of taking only the confirmed cases because the field Total cases was totally incoherent. The values where not always increasing the next day. Even if this is possible i.e suspected cases becomes healthy, it didn't look very acurate. Since both data for cases and death were daily data we just had to aggregate by month using a mean.\n",
    "    \n",
    "    -For the Sierra Leone data: We chose the \"new confirmed\" and the \"death confirmed\" variables. For pretty much the same reasons as above. It looked to us that it was the most relevant and consistant data. \n",
    "    The main difference is that in this case the cases value are daily values and the deaths value are cumulative values. Therefore we worked with two different dataframe in order to aggregate the first with a mean fonction and the second with a max fonction, then minus the precedent value (last month), then divided by 30 in order to have a daily average. We will discuss the efficiency of this method in the results discussion.\n",
    "    \n",
    "    -For the Liberia data: This set of data was the worse. Incoherence as exposed above (with the wrong row), new fields added, other deleted, or some other juste not filled anymore. In order to have, in our opinion, the best data we decided to use two different \"set of values\" depending on the date. For daily report until 2014-10-08 we sumed the fields \"Total cases suspected\", \"Total cases probable\" and \"Total cases confirmed\". For the following dates we used the fields \"Cumulative ... cases\". This choice was mainly motivated by the error present from the daily report of 2014-12-04, where the rows are wrongly filled. And since the cumulative field is not present in the early daily reports it was either drop the data from the 4 december or used two different strategies and then merge. We decided to merge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'g' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d96a88c4ec8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msierra\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'variable'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Variable'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'g' is not defined"
     ]
    }
   ],
   "source": [
    "#Test and vizualise the shape of the data\n",
    "from IPython.display import display\n",
    "from datetime import date\n",
    "guinea = DATA_FOLDER+'/ebola/guinea_data/*.csv'\n",
    "liberia = DATA_FOLDER+'/ebola/liberia_data/*.csv'\n",
    "sierra = DATA_FOLDER+'/ebola/sl_data/*.csv'\n",
    "\n",
    "def get_daily_av(group):\n",
    "    return {'dailyav': (group.max()-group.min())/group.count() }\n",
    "\n",
    "def parse_date(data):\n",
    "    return data.Date.map(lambda x: '-'.join(str(x).split('-')[:2]))\n",
    "\n",
    "s = pd.read_csv(glob.glob(sierra)[0])\n",
    "for file in glob.glob(sierra)[1:]:\n",
    "    s = pd.concat([s,pd.read_csv(file)],axis = 0)\n",
    "frames = [s,g,l]\n",
    "s = s.rename(index=str,columns={'date':'Date','variable':'Variable'})\n",
    "\n",
    "s.Date = pd.to_datetime(s.Date)\n",
    "\n",
    "s['Country'] = 'Sierra-Leone'\n",
    "#Create and clean the DataFrame for the cases\n",
    "s_cases = s[s['Variable'].isin(['new_confirmed'])].copy()\n",
    "s_cases.Variable = 'New cases of confirmed'\n",
    "s_cases.Date = parse_date(s_cases)\n",
    "s_cases = s_cases.set_index(['Country','Date','Variable'])['National']\n",
    "s_cases =pd.DataFrame(s_cases).fillna(0)\n",
    "s_cases = s_cases.astype('int64')\n",
    "s_cases=s_cases.groupby(level=[0,1,2]).mean()\n",
    "s_cases.columns = ['Daily Average']\n",
    "\n",
    "#Create and clean the DataFrame for the deaths\n",
    "s_death = s[s['Variable'].isin(['death_confirmed'])].copy()\n",
    "s_death.Variable = 'New deaths confirmed'\n",
    "s_death.Date = parse_date(s_death)\n",
    "s_death = s_death.set_index(['Country','Date','Variable'])['National']\n",
    "s_death =pd.DataFrame(s_death).fillna(0)\n",
    "s_death = s_death.astype('int64')\n",
    "s_death=s_death.groupby(level=[0,1,2]).max()\n",
    "lastmonth_death = 0\n",
    "for value in s_death.index:\n",
    "    curr = s_death.loc[value].copy()\n",
    "    new_val = curr- lastmonth_death\n",
    "    lastmonth_death = curr \n",
    "    s_death.loc[value] = new_val/30.0\n",
    "s_death.columns = ['Daily Average']\n",
    "\n",
    "#Merges the two DataFrame to have the DataFrame of the country Sierra-Leone\n",
    "s = pd.merge(s_cases.reset_index(), s_death.reset_index(), how= 'outer')\n",
    "s = s.set_index(['Country','Date','Variable'])\n",
    "\n",
    "#Create the DataFrame of the country Guinea\n",
    "g = pd.read_csv(glob.glob(guinea)[0])\n",
    "for file in glob.glob(guinea)[1:]:\n",
    "    g = pd.concat([g,pd.read_csv(file)],axis = 0)\n",
    "g = g.rename(columns={'Description':'Variable'})\n",
    "g.Date = pd.to_datetime(g.Date)\n",
    "g['Country'] = 'Guinea'\n",
    "g = g[g['Variable'].isin(['New cases of confirmed','New deaths registered'])]\n",
    "g = g.fillna(0)\n",
    "g.Date = parse_date(g)\n",
    "g = g.set_index(['Country','Date','Variable']).astype('int64')['Totals']\n",
    "g = g.groupby(level=[0,1,2]).mean()\n",
    "g = pd.DataFrame(g)\n",
    "g.columns = ['Daily Average']\n",
    "\n",
    "#Read the data of the Liberia folder and create a DataFrame\n",
    "l = pd.read_csv(glob.glob(liberia)[0])\n",
    "for file in glob.glob(liberia)[1:]:\n",
    "    l = pd.concat([l,pd.read_csv(file)], axis = 0)\n",
    "l.Date = pd.to_datetime(l.Date)\n",
    "l['Country'] = 'Liberia'\n",
    "l = l[['Country', 'Date', 'Variable','National']]\n",
    "l_death = l[l['Variable'].isin(['Newly reported deaths'])].copy()\n",
    "##Create and clean the DataFrame for the deaths\n",
    "\n",
    "l_death.Date = parse_date(l_death)\n",
    "l_death = l_death.dropna(axis = 0)\n",
    "l_death = l_death.set_index(['Country', 'Date','Variable' ])['National']\n",
    "l_death = l_death.groupby(level=[0,1,2]).mean()\n",
    "l_death = pd.DataFrame(l_death)\n",
    "l_death.columns = ['Daily Average']\n",
    "\n",
    "#Create and clean the DataFrame for the cases\n",
    "l_cases = l[l['Variable'].isin(['Cumulative confirmed, probable and suspected cases',\n",
    "                               'Cumulative (confirmed + probable + suspects)'])].copy()\n",
    "l_cases.Variable = 'New cases(confirmed, probable, suspecet)'\n",
    "l_test= l[l['Variable'].isin(['Total suspected cases','Total probable cases',\n",
    "                               'Total confirmed cases'])].copy()\n",
    "l_test = l_test[l_test.Date < date(2014, 10, 8) ]\n",
    "l_test.Variable = 'New cases(confirmed, probable, suspeced)'\n",
    "l_test = l_test.groupby(['Country', 'Date', 'Variable']).sum()\n",
    "l_m = pd.merge(l_cases.reset_index(), l_test.reset_index(), how= 'outer')\n",
    "\n",
    "l_m.Date = parse_date(l_m)\n",
    "l_m = l_m.set_index(['Country','Date', 'Variable'])['National'].sort_index(level=1)\n",
    "l_m = l_m.groupby(level=[0,1,2]).max()\n",
    "l_m = pd.DataFrame(l_m)\n",
    "lastmonth_cases = 0\n",
    "for value in l_m.index:\n",
    "    curr = l_m.loc[value].copy()\n",
    "    new_val = curr- lastmonth_cases\n",
    "    lastmonth_cases = curr \n",
    "    l_m.loc[value] = new_val/30.0\n",
    "l_m.columns = ['Daily Average']\n",
    "l = pd.merge(l_death.reset_index(), l_m.reset_index(), how= 'outer')\n",
    "l = l.set_index(['Country','Date','Variable'])\n",
    "l.sort_index(level=1)\n",
    "\n",
    "m = pd.merge(s.reset_index(),(pd.merge(g.reset_index(),l.reset_index(),how= 'outer')),how='outer')\n",
    "m = m.set_index(['Country', 'Date', 'Variable'])\n",
    "m=m.sort_index(level=[0,1])\n",
    "m.loc[('Liberia', '2014-12', 'New cases(confirmed, probable, suspecet)')] = m.loc[('Liberia', '2014-12', 'New cases(confirmed, probable, suspecet)')].copy()*3\n",
    "m.loc[('Sierra-Leone', '2014-12', 'New deaths confirmed')] = m.loc[('Sierra-Leone', '2014-12', 'New deaths confirmed')].copy()*3\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Results discussion \n",
    "\n",
    "At first glance we are quite happy with the results. Our computation doesn't lead to any negative values or NaN or any other absurde values. All the daily averages seems to be in the same range.<br>\n",
    "However we can still makes some comments.<br>\n",
    "First of all, some data take the value of confirmed, probable and suspected cases/deaths and some others take only the confirmed cases/deaths. We could argue against this choice but we thought that is was the best way or the most informativ way of representing the data, regarding the data sets.<br>\n",
    "The seconds comments that we could do is that daily values are better than cumulative values. With daily values we can aggregate them using a mean which is quite robust, its is not too dependant on the missing values or the report dates. On the other side cumulative value are dependant to those things especially on the dates. Imagine the case where for one month only one report from the 3rd day of this month is available. We would have took the value of the third day as the max value from this month then we would have substracted the max value of the last month and then weighted it by the number of days in one month. Which would have lead to a to small value to be representative. However in our dataset most month data that used cumulative values had report for advanced day of the month. Except for the month 12 for the \"New deaths confirmed\" of the Sierra-Leone data and the \"New cases(confirmed, probable, suspecet)\" also of the month 12 of the Liberia data, where the reports stop around the tenth. Therefore for those 2 cases, we multiplied the values by 3 since it's a daily average.<br>\n",
    "The last remark will be that for the month of december in the Liberia data we can see that there is no value of the \"Newly reported deaths\". It's is juste because this field had no data for this month.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. RNA Sequences\n",
    "\n",
    "In the `DATA_FOLDER/microbiome` subdirectory, there are 9 spreadsheets of microbiome data that was acquired from high-throughput RNA sequencing procedures, along with a 10<sup>th</sup> file that describes the content of each. \n",
    "\n",
    "Use pandas to import the first 9 spreadsheets into a single `DataFrame`.\n",
    "Then, add the metadata information from the 10<sup>th</sup> spreadsheet as columns in the combined `DataFrame`.\n",
    "Make sure that the final `DataFrame` has a unique index and all the `NaN` values have been replaced by the tag `unknown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Class War in Titanic\n",
    "\n",
    "Use pandas to import the data file `Data/titanic.xls`. It contains data on all the passengers that travelled on the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(filename=DATA_FOLDER+'/titanic.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following questions state clearly your assumptions and discuss your findings:\n",
    "1. Describe the *type* and the *value range* of each attribute. Indicate and transform the attributes that can be `Categorical`. \n",
    "2. Plot histograms for the *travel class*, *embarkation port*, *sex* and *age* attributes. For the latter one, use *discrete decade intervals*. \n",
    "3. Calculate the proportion of passengers by *cabin floor*. Present your results in a *pie chart*.\n",
    "4. For each *travel class*, calculate the proportion of the passengers that survived. Present your results in *pie charts*.\n",
    "5. Calculate the proportion of the passengers that survived by *travel class* and *sex*. Present your results in *a single histogram*.\n",
    "6. Create 2 equally populated *age categories* and calculate survival proportions by *age category*, *travel class* and *sex*. Present your results in a `DataFrame` with unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
